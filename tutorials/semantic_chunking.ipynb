{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Chunking with Chonkie and Model2Vec**\n",
    "\n",
    "Semantic chunking is a task of identifying the semantic boundaries of a piece of text. In this tutorial, we will use the [Chonkie](https://github.com/bhavnicksm/chonkie) library to perform semantic chunking on the book War and Peace. Chonkie is a library that provides a lightweight and fast solution to semantic chunking using pre-trained models. It supports our [potion models](https://huggingface.co/collections/minishlab/potion-6721e0abd4ea41881417f062) out of the box, which we will be using in this tutorial.\n",
    "\n",
    "After chunking our text, we will be using [Vicinity](https://github.com/MinishLab/vicinity), a lightweight nearest neighbors library, to create an index of our chunks and query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (3.1.0)\n",
      "Requirement already satisfied: model2vec in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (2.1.3)\n",
      "Requirement already satisfied: tqdm in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (4.67.0)\n",
      "Requirement already satisfied: vicinity in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (0.2.1)\n",
      "Requirement already satisfied: xxhash in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: filelock in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: packaging in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: aiohttp in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from datasets) (3.11.7)\n",
      "Requirement already satisfied: tokenizers>=0.20 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from model2vec) (0.20.3)\n",
      "Requirement already satisfied: jinja2 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from model2vec) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from model2vec) (65.5.0)\n",
      "Requirement already satisfied: safetensors in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from model2vec) (0.4.5)\n",
      "Requirement already satisfied: rich in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from model2vec) (13.9.4)\n",
      "Requirement already satisfied: orjson in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from vicinity) (3.10.11)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from jinja2->model2vec) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich->model2vec) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from rich->model2vec) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->model2vec) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/thomasvandongen/.pyenv/versions/3.10.12/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "!pip install datasets model2vec numpy tqdm vicinity\n",
    "\n",
    "# Import the necessary libraries\n",
    "import random \n",
    "import re\n",
    "import requests\n",
    "from time import perf_counter\n",
    "from chonkie import SDPMChunker\n",
    "from model2vec import StaticModel\n",
    "from vicinity import Vicinity\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading and pre-processing**\n",
    "\n",
    "First, we will download War and Peace and apply some basic pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for War and Peace on Project Gutenberg\n",
    "url = \"https://www.gutenberg.org/files/2600/2600-0.txt\"\n",
    "\n",
    "# Download the book\n",
    "response = requests.get(url)\n",
    "book_text = response.text\n",
    "\n",
    "def preprocess_text(text: str, min_length: int = 5):\n",
    "    \"\"\"Basic text preprocessing function.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\r\", \" \")\n",
    "    sentences = re.findall(r'[^.!?]*[.!?]', text)\n",
    "    # Filter out sentences shorter than the specified minimum length\n",
    "    filtered_sentences = [sentence.strip() for sentence in sentences if len(sentence.split()) >= min_length]\n",
    "    # Recombine the filtered sentences\n",
    "    return ' '.join(filtered_sentences)\n",
    "\n",
    "# Preprocess the text\n",
    "book_text = preprocess_text(book_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking with Chonkie**\n",
    "\n",
    "Next, we will use Chonkie to chunk our text into semantic chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 7261\n",
      "Time taken: 2.201361084007658\n"
     ]
    }
   ],
   "source": [
    "# Initialize a SemanticChunker from Chonkie with the potion-base-8M model\n",
    "chunker = SDPMChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",\n",
    "    similarity_threshold=0.3\n",
    ")\n",
    "\n",
    "# Chunk the text\n",
    "time = perf_counter()\n",
    "chunks = chunker.chunk(book_text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Time taken: {perf_counter() - time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we chunked the entirety of War and Peace in ~2 seconds. Not bad! Let's look at some example chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " And what role is your young monarch  playing in that monstrous crowd? \n",
      "\n",
      " How can you chuck it in like that or shove it under the cord  where it’ll get rubbed? \n",
      "\n",
      " The general’s face clouded, his lips quivered and trembled. He took  out a notebook, hurriedly scribbled something in pencil, tore out the  leaf, gave it to Kozlóvski, stepped quickly to the window, and threw  himself into a chair, gazing at those in the room as if asking, “Why  do they look at me? ” Then he lifted his head, stretched his neck as  if he intended to say something, but immediately, with affected  indifference, began to hum to himself, producing a queer sound which  immediately broke off. \n",
      "\n",
      " “I like your being businesslike about  it. ”    And patting Berg on the shoulder he got up, wishing to end the  conversation. But Berg, smiling pleasantly, explained that if he did not  know for certain how much Véra would have and did not receive at least  part of the dowry in advance, he would have to break matters off. “Because, consider, Count—if I allowed myself to marry now  without having definite means to maintain my wife, I should be acting  badly. ”    The conversation ended by the count, who wished to be generous and to  avoid further importunity, saying that he would give a note of hand  for eighty thousand rubles. Berg smiled meekly, kissed the count on the  shoulder, and said that he was very grateful, but that it was impossible  for him to arrange his new life without receiving thirty thousand in  ready money. “Or at least twenty thousand, Count,” he added, “and  then a note of hand for only sixty thousand. ”    “Yes, yes, all right! ” said the count hurriedly. “Only excuse me,  my dear fellow, I’ll give you twenty thousand and a note of hand for  eighty thousand as well. ”            CHAPTER XII    Natásha was sixteen and it was the year 1809, the very year to which  she had counted on her fingers with Borís after they had kissed four  years ago. Since then she had not seen him. Before Sónya and her  mother, if Borís happened to be mentioned, she spoke quite freely of  that episode as of some childish, long-forgotten matter that was not  worth mentioning. But in the secret depths of her soul the question  whether her engagement to Borís was a jest or an important, binding  promise tormented her. \n",
      "\n",
      " Borís came to the Rostóvs’ box, received their congratulations very  simply, and raising his eyebrows with an absent-minded smile conveyed to  Natásha and Sónya his fiancée’s invitation to her wedding, and  went away. Natásha with a gay, coquettish smile talked to him, and  congratulated on his approaching wedding that same Borís with whom  she had formerly been in love. In the state of intoxication she was in,  everything seemed simple and natural. The scantily clad Hélène smiled at everyone in the same way, and  Natásha gave Borís a similar smile. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a few example chunks\n",
    "for _ in range(5):\n",
    "    chunk = random.choice(chunks)\n",
    "    print(chunk.text, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those look good. Next, let's create a vector search index with Vicinity and Model2Vec.\n",
    "\n",
    "**Creating a vector search index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1.6793621249962598\n"
     ]
    }
   ],
   "source": [
    "# Initialize an embedding model and encode the chunk texts\n",
    "time = perf_counter()\n",
    "model = StaticModel.from_pretrained(\"minishlab/potion-base-8M\")\n",
    "chunk_texts = [chunk.text for chunk in chunks]\n",
    "chunk_embeddings = model.encode(chunk_texts)\n",
    "\n",
    "# Create a Vicinity instance\n",
    "vicinity = Vicinity.from_vectors_and_items(vectors=chunk_embeddings, items=chunk_texts)\n",
    "print(f\"Time taken: {perf_counter() - time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! We embedded all our chunks and created an in index in ~1.5 seconds. Now that we have our index, let's query it with some queries.\n",
    "\n",
    "**Querying the index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Napoleon\n",
      "--------------------------------------------------\n",
      " Why, that must be  Napoleon’s own. \n",
      "\n",
      " That Napoleon  has left Moscow? \n",
      "\n",
      " Napoleon was to enter the  town next day. \n",
      "\n",
      "Query: The battle of Austerlitz\n",
      "--------------------------------------------------\n",
      " I remember his limited, self-satisfied face on the  field of Austerlitz. \n",
      "\n",
      " That  city is taken; the Russian army suffers heavier losses than the opposing  armies had suffered in the former war from Austerlitz to Wagram. \n",
      "\n",
      " Behave as you did at  Austerlitz, Friedland, Vítebsk, and Smolénsk. \n",
      "\n",
      "Query: Paris\n",
      "--------------------------------------------------\n",
      " “I have been in Paris. \n",
      "\n",
      " A man who doesn’t know Paris  is a savage. You can tell a Parisian two leagues off. Paris is Talma, la  Duchénois, Potier, the Sorbonne, the boulevards,” and noticing that  his conclusion was weaker than what had gone before, he added quickly:  “There is only one Paris in the world. You have been to Paris and have  remained Russian. \n",
      "\n",
      " It rises again from the same  point as before—Paris. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Napoleon\", \"The battle of Austerlitz\", \"Paris\"]\n",
    "for query in queries:\n",
    "    print(f\"Query: {query}\\n{'-' * 50}\")\n",
    "    query_embedding = model.encode(query)\n",
    "    results = vicinity.query(query_embedding, k=3)[0]\n",
    "\n",
    "    for result in results:\n",
    "        print(result[0], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indeed look like relevant chunks, nice! That's it for this tutorial. We were able to chunk, index, and query War and Peace in about 3.5 seconds using Chonkie, Vicinity, and Model2Vec. Lightweight and fast, just how we like it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
