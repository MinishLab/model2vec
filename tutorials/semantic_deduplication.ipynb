{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Deduplication with Model2Vec**\n",
    "\n",
    "In this tutorial, we’ll explore how Model2Vec can help identify duplicates in text data that traditional exact matching would miss. While exact matching works for identical texts, it fails to detect near-duplicates—documents that may differ slightly in wording but convey the same meaning. Using Model2Vec, we embed documents into vectors and measure their similarity. This allows us to catch both exact and semantic duplicates, improving the quality of our dataset. With Model2Vec’s speed and efficiency, we can very efficiently perform deduplication on large datasets, ensuring cleaner, more robust data for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets model2vec reach numpy wordllama tqdm datasketch\n",
    "from datasets import load_dataset\n",
    "from model2vec import StaticModel\n",
    "from reach import Reach\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from difflib import ndiff\n",
    "from wordllama import WordLlama\n",
    "from time import perf_counter\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and dataset\n",
    "model = StaticModel.from_pretrained(\"minishlab/M2V_base_output\")\n",
    "ds = load_dataset(\"ag_news\")[\"train\"]\n",
    "texts = ds['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to find exact matches in the dataset as a baseline. Then, we will use Model2Vec to identify semantic duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen = set()\n",
    "deduplicated_text_indices = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    if text not in seen:\n",
    "        deduplicated_text_indices.append(i)\n",
    "        seen.add(text)\n",
    "\n",
    "len(deduplicated_text_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, we find no duplicate instances using exact string matching. Now, let's use Model2Vec to embed our documents and identify duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:02<00:00, 45.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Encode texts into embeddings\n",
    "embedding_matrix = model.encode(texts, show_progressbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate(embedding_matrix: np.ndarray, threshold: float, batch_size: int = 1024) -> tuple[np.ndarray, dict[int, int]]:\n",
    "    \"\"\"\n",
    "    Deduplicate embeddings and return the deduplicated indices and a mapping of removed indices to their corresponding original indices.\n",
    "    \n",
    "    :param embedding_matrix: The embeddings to deduplicate.\n",
    "    :param threshold: The similarity threshold to use for deduplication.\n",
    "    :param batch_size: The batch size to use for similarity computation.\n",
    "    :return: A tuple containing the deduplicated indices and a dictionary mapping removed indices to original indices.\n",
    "    \"\"\"\n",
    "    reach = Reach(vectors=embedding_matrix, items=[str(i) for i in range(len(embedding_matrix))])\n",
    "    \n",
    "    # Use a set for deduplicated indices and keep track of duplicates\n",
    "    deduplicated_indices = set(range(len(embedding_matrix)))  # Start with all indices as deduplicated\n",
    "    duplicate_to_original_mapping = {}\n",
    "\n",
    "    results = reach.nearest_neighbor_threshold(\n",
    "        embedding_matrix, \n",
    "        threshold=threshold, \n",
    "        batch_size=batch_size, \n",
    "        show_progressbar=True\n",
    "    )\n",
    "    \n",
    "    # Process duplicates\n",
    "    for i, similar_items in enumerate(tqdm(results)):\n",
    "        if i not in deduplicated_indices:\n",
    "            continue  # Skip already marked duplicates\n",
    "\n",
    "        # Similar items are returned as (index, score), we are only interested in the index\n",
    "        similar_indices = [int(item[0]) for item in similar_items if int(item[0]) != i]\n",
    "        \n",
    "        # Mark similar documents as duplicates and map them to the original\n",
    "        for sim_idx in similar_indices:\n",
    "            if sim_idx in deduplicated_indices:\n",
    "                deduplicated_indices.remove(sim_idx)\n",
    "                duplicate_to_original_mapping[sim_idx] = i  # Map duplicate to original\n",
    "\n",
    "    return np.array(list(deduplicated_indices)), duplicate_to_original_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:25<00:00,  4.64it/s]\n",
      "100%|██████████| 120000/120000 [00:00<00:00, 679800.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deduplicated docs: 118769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Deduplicate (with a high threshold)\n",
    "deduplicated_indices, duplicate_to_original_mapping = deduplicate(embedding_matrix, threshold=0.99)\n",
    "print(f\"Number of deduplicated docs: {len(deduplicated_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Model2Vec, we find > 1000 duplicates with a very high threshold, in < 30 seconds. Now, let's look at a few examples to see if these are indeed duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "Duplicate text:\n",
      "Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market this week during the depth of the\\summer doldrums.\n",
      "Differences:\n",
      "- next + this\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.\n",
      "Duplicate text:\n",
      "Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market this week during the depth of the  summer doldrums.\n",
      "Differences:\n",
      "- next + this\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "Phelps, Thorpe Advance in 200 Freestyle ATHENS, Greece - Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and then found out he had been added to the American team for the evening's 400 freestyle relay final. Phelps' rivals Ian Thorpe and Pieter van den Hoogenband and teammate Klete Keller were faster than the teenager in the 200 free preliminaries...\n",
      "Duplicate text:\n",
      "Phelps, Thorpe Advance in 200 Freestyle ATHENS, Greece - Michael Phelps took care of qualifying for the Olympic 200-meter freestyle semifinals Sunday, and then found out he had been added to the American team for the evening's 400 freestyle relay final.    Phelps' rivals Ian Thorpe and Pieter van den Hoogenband and teammate Klete Keller were faster than the teenager in the 200 free preliminaries...\n",
      "Differences:\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "Government Spending Up Sharply Locally  Federal procurement spending in the Washington area rose last year at its highest rate since the 1980s, according to a study to be released today, creating tens of thousands of jobs and increasing economic growth disproportionately in Northern Virginia.\n",
      "Duplicate text:\n",
      "Government Spending Up Sharply Locally Federal procurement spending in the Washington area rose last year at its highest rate since the 1980s, according to a study to be released today, creating tens of thousands of jobs and increasing economic growth disproportionately in Northern Virginia.\n",
      "Differences:\n",
      "\n",
      "--------------------------------------------------\n",
      "Original text:\n",
      "F.B.I. Goes Knocking for Political Troublemakers The F.B.I. has been questioning demonstrators in an effort to forestall violent protests at the Republican National Convention.\n",
      "Duplicate text:\n",
      "F.B.I. Goes Knocking for Political Troublemakers The F.B.I. has been questioning demonstrators in an effort to forestall violent protests at the Republican convention.\n",
      "Differences:\n",
      "- National - Convention. + convention.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def display_word_differences(x: str, y: str) -> str:\n",
    "    diff = ndiff(x.split(), y.split())\n",
    "    return \" \".join([f\"{word}\" for word in diff if word.startswith(('+', '-'))])\n",
    "\n",
    "# Show a few duplicates with their originals, highlighting word-level differences\n",
    "num_examples = 5\n",
    "for duplicate_idx, original_idx in list(duplicate_to_original_mapping.items())[:num_examples]:\n",
    "    print(f\"Original text:\\n{texts[original_idx]}\")\n",
    "    print(f\"Duplicate text:\\n{texts[duplicate_idx]}\")\n",
    "    print(\"Differences:\")\n",
    "    print(display_word_differences(texts[original_idx], texts[duplicate_idx]))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found texts do indeed seem to be duplicates, nice! In a normal workflow where we use Model2Vec to embed our documents, deduplication our training corpus is essentially free. This gives us an easy to use, easy to integrate, fast way to deduplicate.\n",
    "\n",
    "For comparison, let's also try a different library (WordLlama), which also uses static embeddings to deduplicate text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = WordLlama.load()\n",
    "\n",
    "time = perf_counter()\n",
    "deduplicated_docs = wl.deduplicate(texts, threshold=0.99)\n",
    "print(f\"Number of deduplicated docs: {len(deduplicated_docs)}\")\n",
    "print(f\"Time taken: {perf_counter() - time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is considerably slower than Model2Vec for encoding + deduplication (43 vs 27 seconds). It also finds less duplicates with the same threshold.\n",
    "\n",
    "As a last comparison, let's use MinHash, a common method for deduplication. We will use the datasketch library to find duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deduplicated docs: 118653\n",
      "Time taken: 56.46521229199425\n"
     ]
    }
   ],
   "source": [
    "def get_minhash(text: str, num_perm: int = 128) -> MinHash:\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def deduplicate_with_minhash(texts: list[str], threshold: float = 0.9) -> list[int]:\n",
    "    \"\"\"\n",
    "    Deduplicate texts using MinHash and return the indices of unique texts.\n",
    "\n",
    "    :param texts: List of texts to deduplicate.\n",
    "    :param threshold: Jaccard similarity threshold for considering texts as duplicates.\n",
    "    :return: List of indices of deduplicated texts.\n",
    "    \"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    deduplicated_text_indices = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        # Generate MinHash for the current text\n",
    "        minhash = get_minhash(text)\n",
    "\n",
    "        # Check if the MinHash is already in the LSH (i.e., if it is a duplicate)\n",
    "        if not lsh.query(minhash):\n",
    "            # If it's not a duplicate, add the MinHash and keep the index\n",
    "            deduplicated_text_indices.append(i)\n",
    "            lsh.insert(i, minhash)\n",
    "\n",
    "    return deduplicated_text_indices\n",
    "\n",
    "\n",
    "time = perf_counter()\n",
    "deduplicated_text_indices = deduplicate_with_minhash(texts)\n",
    "print(f\"Number of deduplicated docs: {len(deduplicated_text_indices)}\")\n",
    "print(f\"Time taken: {perf_counter() - time}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model2Vec is again much faster, with 27 seconds vs 56 seconds for MinHash. The number of found duplicates is roughly the same using the default settings for MinHash."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
